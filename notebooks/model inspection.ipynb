{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection: extraction and analysis of patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model as lm\n",
    "import pickle\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paths\n",
    "\n",
    "root_path = './../'\n",
    "\n",
    "# path to data, random splits, cell list\n",
    "data_base_path = root_path+'data/'\n",
    "split_base_path = data_base_path+'- splits/'\n",
    "cell_list_path = data_base_path+'cells.csv'\n",
    "name_base_path = data_base_path+'names.csv'\n",
    "gene_info_path = data_base_path+'GeneFile.txt'\n",
    "\n",
    "# path to folder where to write results\n",
    "score_base_path = root_path+'scores/'\n",
    "try:\n",
    "    os.mkdir(score_base_path)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# suffixes\n",
    "data_suffix = 'stripped/'\n",
    "split_suffix = 'iteration_'\n",
    "X_tail = 'X.csv'\n",
    "T_tail = 'T.csv'\n",
    "\n",
    "# prepare list of data folders where to read\n",
    "# so to feed all the considered combinations\n",
    "# of input formats\n",
    "formats = ['- broad', '- broad narrow', '- gapped', '- gapped broad', '- gapped narrow', '- narrow']\n",
    "folders_by_format = list()\n",
    "for form in formats:\n",
    "    base = data_base_path + form + '/'\n",
    "    folders, cells = load_cell_paths(cell_list_path, base, suffix='/'+data_suffix)\n",
    "    folders_by_format.append(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target epigenomes and gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will extract and inspect _ShallowChrome_ regulative patterns for gene `PAX5` across epigenomes `H1(-hESC)`, `GM12878` and `K562`. Just change the target values below to extract patterns for different genes and epigenomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## targets\n",
    "\n",
    "# target epigenome(s)\n",
    "target_cells = [\n",
    "    'E003',  # H1(-hESC)\n",
    "    'E116',  # GM12878\n",
    "    'E123']  # K562\n",
    "cs = [np.where(cells==cell)[0][0] for cell in target_cells]\n",
    "\n",
    "# target gene\n",
    "target = 'ENSG00000196092_PAX5'\n",
    "\n",
    "# where is the target gene?\n",
    "with open(gene_info_path) as handle:\n",
    "    lines = handle.readlines()\n",
    "l = 0\n",
    "current_name = target + '_'\n",
    "found = False\n",
    "while l < len(lines) and not found:\n",
    "    line = lines[l]\n",
    "    current_name = line.strip().split('\\t')[-1]\n",
    "    if current_name == target:\n",
    "        found = True\n",
    "    l += 1\n",
    "assert found\n",
    "position = l - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data split, along with model and fitting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the standard data split employed in _DeepChrome_: train, validation and test sets are simply obtained by splitting the list of genes sequentially into three parts, without reshuffling. This split suits our needs, as target gene `PAX5` belongs to the test set, so it is not used to estimate _ShallowChrome_ parameters.\n",
    "\n",
    "NB: l2-penalty coefficient `C = +∞` effectively enforces no penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and fitting parameters\n",
    "penalty='l2'\n",
    "C = + np.inf\n",
    "solver = 'lbfgs'\n",
    "max_iter = 6000\n",
    "multi_class = 'multinomial'\n",
    "random_state = 666\n",
    "\n",
    "# split\n",
    "ref_iter = 0\n",
    "split = load_split(split_base_path, split_suffix, ref_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train _ShallowChrome_ on the selected split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...  and select best input format. Set `target_only = True` to only train on the target epigenomes. With `target_only = False` training and input format selection is performed on _all_ epigenomes. This is required to perform the comparison with _ChromHMM_ emission patterns in notebook `model validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training!\n",
    "    \n",
    "target_only = True\n",
    "if target_only:\n",
    "    loop_cells = target_cells\n",
    "    loop_indexes = cs\n",
    "else:\n",
    "    loop_cells = cells\n",
    "    loop_indexes = list(range(len(cells)))\n",
    "\n",
    "bests = dict()\n",
    "for c, cell in zip(loop_indexes, loop_cells):\n",
    "    \n",
    "    # print advancement message\n",
    "    print '\\n\\n>> cell {0}...'.format(cell)\n",
    "\n",
    "    # set score folder(s)\n",
    "    score_folder = score_base_path+str(cell)\n",
    "    try:\n",
    "        os.mkdir(score_folder)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # define score matrices\n",
    "    S_val = np.ndarray((len(formats), 1))\n",
    "    S_test = np.ndarray((len(formats), 1))\n",
    "\n",
    "    # loop for data formats\n",
    "    for f, form in enumerate(formats):\n",
    "\n",
    "        # print advancement message\n",
    "        print '\\r\\tformat: {0}       '.format(form),\n",
    "\n",
    "        # set score folder for format\n",
    "        score_folder_format = score_folder+'/'+form+'/'\n",
    "        try:\n",
    "            os.mkdir(score_folder_format)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # retrieve data\n",
    "        folder = folders_by_format[f][c]\n",
    "        X = np.loadtxt(folder+X_tail, delimiter=',')\n",
    "        T = binarize_target(np.loadtxt(folder+T_tail, delimiter=','))\n",
    "\n",
    "        # instantiate model\n",
    "        model = lm.LogisticRegression(penalty=penalty, C=C, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class)\n",
    "\n",
    "        # fit model\n",
    "        cache_model_at = score_folder_format+'model_C_'+str(C)+'_iter_'+str(ref_iter)+'.pkl'\n",
    "        S_val[f,0], S_test[f,0] = fit_and_score(model, X, T, split, cache_model_at)\n",
    "\n",
    "    val_scores = S_val[:, 0]\n",
    "    best = np.argmax(val_scores)\n",
    "    bests[cell] = best\n",
    "    \n",
    "best_formats = {cell: formats[bests[cell]] for cell in loop_cells}\n",
    "\n",
    "# save best formats for future analyses\n",
    "with open(score_base_path+'bests_{}.pkl'.format(0), 'wb') as handle:\n",
    "    pickle.dump(best_formats, handle)\n",
    "    \n",
    "print(' done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract _ShallowChrome_ patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reload best trained models and extract their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to only target cells and best found input format\n",
    "folders, cells = load_cell_paths(cell_list_path, data_base_path, suffix='/'+data_suffix, best_formats=best_formats, cells=np.asarray(target_cells))\n",
    "data_dict = dict()\n",
    "for c, cell in enumerate(cells):\n",
    "    X = np.loadtxt(folders[c]+X_tail, delimiter=',')\n",
    "    T = np.loadtxt(folders[c]+T_tail, delimiter=',')\n",
    "    data_dict[cell] = (X, T)\n",
    "\n",
    "# load models for the target cells and retrieve weights\n",
    "models = dict()\n",
    "weights = dict()\n",
    "print 'extracting model weights... '\n",
    "for c, cell in enumerate(cells):\n",
    "    \n",
    "    print '\\r\\t>> cell {0}...  '.format(cell),\n",
    "    score_folder = score_base_path+str(cell)\n",
    "    score_folder_format = score_folder+'/'+best_formats[cell]+'/'\n",
    "    cache_model_at = score_folder_format+'model_C_'+str(C)+'_iter_'+str(0)+'.pkl'\n",
    "    with open(cache_model_at, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    # retrieve model and weights\n",
    "    models[cell] = model\n",
    "    weights[cell] = model.coef_\n",
    "\n",
    "print ' done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract patterns for target gene\n",
    "\n",
    "inputs = dict()\n",
    "labels = dict()\n",
    "patterns = dict()\n",
    "predictions = dict()\n",
    "\n",
    "print 'extracting pattern for gene {0}... '.format(target)\n",
    "for c, cell in enumerate(cells):\n",
    "    \n",
    "    print '\\r\\t>> cell {0}...   '.format(cell),\n",
    "    \n",
    "    # get score folder\n",
    "    score_folder = score_base_path+str(cell)+'/'\n",
    "    \n",
    "    # retrieve data\n",
    "    x = data_dict[cell][0][position]\n",
    "    t = data_dict[cell][1][position]\n",
    "    inputs[cell] = x\n",
    "    labels[cell] = int(t >= np.median(data_dict[cell][1]))\n",
    "    predictions[cell] = models[cell].predict(x.reshape([1, -1]))\n",
    "    \n",
    "    # compute and dump pattern\n",
    "    pattern = weights[cell] * x\n",
    "    patterns[cell] = pattern\n",
    "    with open(score_folder + 'pattern_{0}_ref_iter_{1}.csv'.format(target, ref_iter), 'wb') as handle:\n",
    "        pickle.dump(pattern, handle)\n",
    "\n",
    "print ' done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the extracted patterns, shadowing out irrelevant predictors – a statistical `z-test` is internally performed over the train set in routine `draw_specific_pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_array = np.loadtxt(name_base_path, delimiter=',', dtype=str)\n",
    "name_dict = {k+1: name_array[k] for k in range(len(name_array))}\n",
    "name_dict[0] = 'intercept'\n",
    "cell_names = ['H1-hESC (OFF)', 'GM12878 (ON)', 'K562 (OFF)']\n",
    "for c, cell in enumerate(cells):\n",
    "    X, T = data_dict[cell]\n",
    "    split = load_split(split_base_path, split_suffix, 0)\n",
    "    X_train = X[split[0]]\n",
    "    y_train = T[split[0]]\n",
    "    draw_specific_pattern(models[cell], \n",
    "                          patterns[cell], \n",
    "                          name_array,\n",
    "                          X_train,\n",
    "                          y_train,\n",
    "                          show_intercept=True, \n",
    "                          absolute=True,\n",
    "                          norm=True,\n",
    "                          label=cell_names[c],\n",
    "                          y_bounds=None,\n",
    "                          legend=True,\n",
    "                          cache_at='./{}_{}.pdf'.format(target, cell),\n",
    "                          show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
